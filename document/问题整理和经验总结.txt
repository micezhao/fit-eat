++fit-eat 开发过程的问题记录和经验总结

第一周（2019/10/21 - 2019/10/27）
已完成任务：
1、通过frp方案实现内网穿透
    1.1 将【服务端frps】 部署到阿里云，作为代理服务器
    1.2 将【客户端frpc】 部署到imac，作为客户端
    *******【服务端frps】操作******
    1.3 修改(添加)服务端的frps.ini的配置 bind_port = 7000 #在服务端通过7000端口与客户端进行绑定
    1.4 启动frps服务
    *******【服务端frps】操作******
    1.5 修改(添加)客户端的frpc.ini的配置 , 此配置的目的，是将内网服务器与外网服务器关联起来
        [common]
        server_addr = 118.190.53.214
        server_port = 7000
    1.6 修改(添加)配置，使得内网服务器的ssh功能可以通过外网访问
        [ssh]
        type = tcp 
        local_ip = 127.0.0.1
        local_port = 22
        remote_port = 6000
    1.7 如上文述，几项重点节点的解释
        [xx]：节点说明，此配置内容不可重复
        type：通信协议类型 常用：tcp/http/https
        local_ip：内网服务器的ip地址，如果某个应用与frpc部署在同一台机器，则为127.0.0.1
        local_port：当前节点的应用对应的本机端口号 
        remote_port：当前应用通过外网暴露的访问端口号
        例子：
        [mysql]
        type = tcp
        local_ip = 127.0.0.1
        local_port = 3306
        remote_port = 3386
    1.8 修改完成后，启动frpc服务，启动完成后，可以监控日志检查代理连接情况，日志中出现如下信息，则表示代理成功
        [ssh] start proxy success
        [redis] start proxy success
        等等
    *******【阿里云服务】操作******
    1.9 阿里云作为外网服务的实际提供者，需要在阿里云控制台打开frpc中的remote_port配置项的端口，以便外网请求可以访问可以通过阿里云的安全控制策略

2、编辑配置清单/第一阶段的功能规划
    2.1 将开发过程中需要用到的相关配置信息，编辑成表格，对网络连接访问的相关信息，和开发环境的相关配置信息进行明确说明。此文件将随着功能开发的过程，持续维护；
    2.2 对第一阶段的功能进行规划，并作为里程碑，用来对照开发过程。

3、kobe系统的功能分析与对象设计
    3.1 理解oauth认证方式：这是一种当下场景的认证方案，意图通过将某个用户已经在其他应用中已经认证过的身份，作为本系统认证凭证，可以避免用户持有并记录过多的认证信息，降低用户方位新应用的门槛
    3.2 理解应用作为微信/支付宝小程序时，当用户访问本小程序时的授权操作的本意是，应用请求微信平台，需要获取当前用户在微信中的相关信息。而这个授权的动作是用户授权微信平台将信息提供给本应用的
    3.3 kobe系统的具备的功能分析：
        3.3.1 实现多渠道方位，多方式认证的支持
        3.3.2 实现同一用户通过不同渠道访问时的账户信息绑定
        3.3.3 高效的生成唯一的用户编号
        3.3.4 实现分布式的身份认证令牌的共享
    3.4 kobe系统的第一阶段功能所需要的对象分析
        3.4.1 本次对象分析，将基本对象与视图对象两种
        3.4.2 基本对象的相关数据，存入sql数据库，视图对象存入nosql数据库

4、基于springcloud技术栈搭建应用体系
    4.1 springboot 版本：2.2.0
    4.2 springcloud  版本：Hoxton.M3
    4.3 maven 构建模式

5、部署jordan/david 应用，使之可以被外网访问到
    5.1 jordan 作为注册中心
    5.2 david 作为配置中心

    ****【注意】****
    关于eureka-client注册的配置方式的区别：
    5.1 通过真实ip地址向eureka-server注册，配置如下：
    eureka:
        instance:
            lease-expiration-duration-in-seconds: 5 #设置心跳的周期间隔(默认90s)[如果5s没响应默认服务宕机]
            lease-renewal-interval-in-seconds: 2  #设置心跳时间间隔(默认30s)
            prefer-ip-address: true       #访问路径变为IP地址
            instance-id: ${spring.application.name}:${spring.cloud.client.ip-address}:${server.port} 
    采用此方式的注册，在eureka-server 的监控台中，将显示这个当前client的真实ip地址。【此方案使用在所有应用在同一套网络环境下】
    5.2、通过hostname的方式向eureka-server注册
     eureka:
        instance:
            hostname：imac-david
    采用此方式的注册，在eureka-server 的监控台中，将显示这个当前client的域名地址。【此方案使用在应用不在同一套网络环境下】
    5.3、通过实验证明：
        3.1 在springcloud体系中，当应用注册到eurka-server后，eureka将会维护一个serviceId与client所在网络地址的映射关系列表；
        3.2 请求达到eurka-server后，会通过映射关系列表，查找被访问的serviceId对应的网络地址，并将请求转发到目标地址；
        3.3 如果客户端之间可以通过注册到eureka的ip地址进行相互请求（即：所有应用在同一套网络环境下），那么请求之间可以正常调用
        3.4 如果应用不再同一个网络环境，那么应用之间的互相调用会失败。虽然可以通过serviceId进行转发，但是由于网络环境屏障，请求无法依然访问到serviceId对应的ip地址
        3.5 此时，就需要采用hostname的方式来注册客户端。同时在消费者所在系统改写host地址，将生产者的hostname与生产者的ip地址，在消费者本机进行绑定。
        3.6 这样一来，相当于在消费者本机进行了dns的解析。

第二周（2019/10/28 - 2019/11/01）
已完成的任务：
1、研究eureka的服务注册流程：
    ********【服务端启动和接受注册的流程】*********
    1.1 eureka-server的启动流程
        1.1.1 EurekaServerInitializerConfiguration 负责提供初始化eureka-server容器的初始化，
              这个类实现了 SmartLifecycle extends Lifecycle 的start()方法，用来在spring启动时加载eureka启动项
        1.1.2 在strat（）方法中完成如下事项：
                    ｜—— eurekaServerBootstrap.contextInitialized (EurekaServerInitializerConfiguration.this.servletContext）将eureka-server初始化到spring的上下文中
        1.1.3 publish(new EurekaRegistryAvailableEvent(getEurekaServerConfig())); 发布服务端可用时间
        1.1.4 EurekaServerInitializerConfiguration.this.running = true; 将EurekaServer的状态设置为运行状态；
        1.1.5 publish(new EurekaServerStartedEvent(getEurekaServerConfig())); 发布服务端启动事件
    1.2 EurekaServer启动后，将向外提供一个接受注册的方法:ApplicationResource.addInstance ,客户端可通过发送请求到 ip:port/eureka/apps/{id} 完成将client注册到EurekaServer中
    1.3 在ApplicationResource.addInstance方法中通过执行PeerAwareInstanceRegistry.registry(info, "true".equals(isReplication));，将完成服务注册的逻辑
        【info是InstanceInfo对象的实例，info由client端提供】
    1.4 registry方法最后落脚到AbstractInstanceRegistry类中的registry方法中，在AbstractInstanceRegistry.registry方法中，
        最后将这个info写入到 ConcurrentHashMap<String, Map<String, Lease<InstanceInfo>>> 中，完成服务的注册步骤。
        【2019-11-04 提问：最外层的key存的是什么】
        这个map维护的是 serviceId 与这个serviceId对应的InstanceInfo的租约信息【Lease】的映射关系。
    1.5 Lease【租约】对象中存放了 InstanceInfo ,evictionTimestamp[失效时间],registrationTimestamp【注册时间】,serviceUpTimestamp【启动时间】等信息
    
    ********【客户端启动和接受注册的流程】*********
    1.6 eureka-client的启动流程
        1.6.1 在client端由EurekaAutoServiceRegistration 作为启动入口，与服务端一样，实现了Lifecycle的start（）方法，在启动时将eureka-client初始化到spring的上下文中
        1.6.2 在strat（）方法中，通过调用EurekaServiceRegistry.register方法来实现向服务端注册的执行逻辑；
    1.7 在调用EurekaServiceRegistry.register的过程中，
                    ｜——首先会通过ApplicationInfoManager【应用管理器】将客户端信息转化为InstanceInfo
                    ｜——此时，client还未完成启动过程，InstanceInfo的初始状态为starting
                    ｜——在启动完成后，会将InstanceInfo设置为up
    1.8 当Instanceinfo的状态为up后，会触发ApplicationInfoManager.StatusChangeListener监听器发起notify通知
    1.9 通知的内容将交给InstanceInfoReplicator.run（）方法中去执行
                    ｜——首先刷新这个instanceInfo的相关状态
                    ｜——调用discoveryClient.register()方法，准备开始进行注册
    1.10 在discoveryClient.register() 方法中 执行 httpResponse = eurekaTransport.registrationClient.register(instanceInfo); 向eureka-server进行注册
    1.11 最后使用EurekaHttpClient接口的AbstractJerseyEurekaHttpClient实现类中的register方法，向ip:port/eureka/apps/{id} 发送请求，发送的内容就是InstanceInfo

    至此：eureka server/client 的启动与注册流程就执行完毕
    同时，为了维护server/client之间的关系，client端将由一个定时任务 scheduleTask定时向sever发送心跳，来维持连接，而在server由Lookup类来轮训当前处于注册列表中的服务
    
2、通过自定义注解+aop的方式实现将mysql数据同步到mongodb的操作
    2.1 关于mongodb的数据库系统的权限控制和连接管理
    2.2 关于对java提供的注解的深入学习
        java中的注解分为元注解与注解（包括：java提供的注解和自定义注解），其中元注解是有java语言提供的，作用是描述或限定自定义注解的注解
        常用的元注解有：
        @Target：用于限定注解的作用位置，例如：@Target({ElementType.METHOD,ElementType.PARAMETER,ElementType.FIELD}
        @Retention:声明注解的保留期，常用：@Retention(RetentionPolicy.RUNTIME)，表示在运行期间jvm可以通过反射获得该注解的相关信息
        @Document:说明被修饰的注解是否需要包含Javadoc中
        @Inherited:描述这个注解可以被继承，假设注解 @AnnoTest被元注解@Inherited修饰，类A 被@AnnoTest修饰，类B继承了A，那么B也被@AnnoTest修饰
        java的提供的注解有：
        @Override，表示当前的方法定义将覆盖超类中的方法。
        @Deprecated，使用了注解为它的元素编译器将发出警告，因为注解@Deprecated是不赞成使用的代码，被弃用的代码。
        @SuppressWarnings,关闭不当编辑器警告信息。
        自定义注解：
        自定义注解是一个特殊的接口，声明的方法是@Interface ，在自定义注解接口中可以定义接口中的方法及其默认值 
        @Target(ElementType.METHOD)
        @Retention(RetentionPolicy.RUNTIME)
        public @interface UseCases{
            public String id();
            public String description() default "no description";
        }
        在运行时可以通过java的反射来获得这个注解的相关信息，
		Annotation[] annotations01 = Class.getAnnotations();
    2.3 将自定义注解作用结合AOP实现功能的扩展（这也是一种装饰器模式）
        2.3.1 自定义一个注解 例如：@ToMongoDB
        2.3.2 准备一个Aspect类
        2.3.3 指定 @Before/@After/@Around 通知注解的value属性为自定义注解 @After(value = "@annotation(ToMongoDB)") 
        2.3.4 在切面的方法中，编写程序逻辑，即可
    2.4 关于springboot的自动化配置：
        自动化配置可以说是springboot框架中最重要的一个功能之一，在了解springboot的自动化配置功能(spring-boot-autoconfigure.jar)之前，首先要了解springboot可以自动化的配置哪些内容
        2.4.1 在spring-boot-autoconfigure.jar的/MATA-INF 维护两个文件 spring.factories 和 spring-autoconfigure-metadata.properties 两个文件
              spring.factories:声明了可以被自动化配置的组件/模块/功能列表
              spring-autoconfigure-metadata.properties:维护了各组件/模块/功能列表的代码入口
        那么为什么需要这个功能呢？
        这就要从spring的历史说起，spring框架具有面向接口编程的特点，这个特点使得spring框架本身具有极强的兼容性和扩展能力。
        当开发者需要在项目中使用到某个中间件时（例如：jdbc，mq，cache）时，只需要编写一个专门的xml配置文件将这个组件与spring框架对接起来，也就是：将Spring框架提供的对应的接口与组件的实现进行匹配
        以向spring框架中集成一个消息中间为例：https://blog.csdn.net/isea533/article/details/84545484
        试想：如果一个工程需要配置诸多中间件时，且在一个多工程聚合的项目中，这种xml配置文件就需要耗费大量的维护成本。
        因此springboot提高了自动化配置的功能，是基于”约定优于配置“的原则，将这些可能会用到的，并且Spring已经提供了实现了的组件进行同一的管理。
        让开发人员只需通过配置文件来声明一些【如连接信息】个性化程度很高的配置项参数，就可以实现中间的集成。这样以来，极大的简化了工程项目的集成难度，让开发者可以更多的将经历投入在业务开发中。
        那Springboot是如何完成自动化配置的呢？
        2.4.2.1 自动配置的入口：main中的SpringApplication.run(xxApplication.Class,args)方法 => args是来自springboot项目通过命令启动时的启动参数，例如 java -jar my-spring-boot.jar --spring.profiles.active=prod 其中 --spring.profiles.active=prod就时args的一部分
        2.4.2.2 自动化配置是通过注解@SpringBootApplication 来进行，@SpringBootApplication -> @Target(ElementType.TYPE) 这个注解作用与类上
        2.4.2.3 可以被自动配置的组件：@SpringBootApplication - >import @EnableAutoConfiguration -> @Import(AutoConfigurationImportSelector.class)
                AutoConfigurationImportSelector.class -> 
                    selectImports -> (
                        getAutoConfigurationEntry - (
                            getAutoConfigurationEntry -> (
                                getCandidateConfigurations -- （ // 获得候选人
                                    SpringFactoriesLoader.loadFactoryNames(
                                      -> loadSpringFactories( // FACTORIES_RESOURCE_LOCATION = "META-INF/spring.factories";
                                          classLoader.getResources(FACTORIES_RESOURCE_LOCATION) : 
					                      ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION));
                                        ) 读取完成后，将需要被自动加载的内容存储在一个 LinkedMultiValueMap<>()中;
                                    ）
                                )
                            )
                        )
                    ) == >> 执行完成后，封装成 AutoConfigurationEntry.class --> AutoConfigurationEntry(Collection<String> configurations, Collection<String> exclusions) 
        2.4.2.4 哪些组件将会被加载 @SpringBootApplication - > @ComponentScan 扫描整个项目中的bean，
                 当工程中引用了spring.factories中的某个组件后，将会被扫描到从而成为一个在项目启动过程总需要被加载的组件
        2.4.2.5 项目启动，开始加载 SpringApplication.class --> 
                run(
                     getRunListeners(
                         getSpringFactoriesInstances(
                             createSpringFactoriesInstances( //并将他们创建成一个 java的实例 
                                 SpringFactoriesLoader.loadFactoryNames() // 从"META-INF/spring.factories" 中获取要加载的配置
                             )
                         )
                     )
                     prepareContext(
                         listeners.contextPrepared(context); //将这些监听器的实例放入转为上下文
                     )
                     ConfigurableListableBeanFactory beanFactory = context.getBeanFactory();
		             beanFactory.registerSingleton("springApplicationArguments", applicationArguments); // 将上下文中的内容变成一个singleton bean
                     listeners.contextLoaded(context); // 启动被监听的监视器
                )
        2.4.2.6 项目启动的过程中，诸多被加载的组件也需要同步开始运行，而这些组件大多需要进行一些额外的配置，例如数据库的连接信息，消息中间的服务端地址
                这些额外的配置通过spring-autoconfigure-metadata.properties文件中指定的组件代码入口上的@EnableConfigurationProperties(xxxProperties.class)
                xxxProperties.class来读取额外的配置信息


       

3、设计模式（一）：
    3.1 装饰器模式：结构型模式
        装饰器模式践行了设计模式大原则中的开闭原则（即：对扩展开发，对修改关闭），其作用在于在不改变原有功能的前提下，通过装饰器对功能提供扩展
        实现装饰器的条件：
        准备过程：
        1、baseClass = > method_1,method_2,...,method_n
        2、decorateClass =>protected arg baseClass , constructor(baseClass),@Override method_1( extendtion method)
        调用方式：
        DecorateClass decorateClass = new DecorateClass(baseClass); // 通过装饰器类对基类进行包装
        decorateClass.method_1() => 此时执行的是被扩展的method_1 的结果 
        装饰器本身最好是一个abstract类，方便被同类型但需要个性化扩展方法的类继承并重写装饰的方法
    3.2 策略模式：


第三周(2019/11/04 - 2019/11/08) 
已完成的任务：
1、学习spring-session组件，以及通过spring session 来实现分布式回话管理
    1.1 session是什么？
        在解释session是什么之前，先了解一下一个网络请求的执行过程。大部分的网络请求都使用http/https协议进行，这种通信协议的具有无状态的
        【无状态性】指：上一个请求和下一个请求之间是没有任何关联，上个请求中的结果不会影响下个请求。
        设想一个情景：即如果 url—1 是负责对请求者身份进行认证，url-2是向用户提供购物功能。基于http协议的无状态性，用户通过url-1完成了身份认证，
        跳转到url-2，但是url-2无法从url-1中获取用户的任何信息，必须要再次进行一次查询操作才能获得用户的相关信息。这样势必会十分影响系统性能。
        为了解决这个问题，首先诞生了【cookie技术】，这种在服务器相应了客户端请求后，将和客户端操作相关的一小部分信息存储与客户端（例如：浏览器中）
        这样一来，就可以将通过上个请求拿到的相关信息留存下来，供下一个请求使用。但是，cookie存在容量和安全问题，因为信息存放于客户端，用户便可以随意修改cookie中存放的信息。
        因此，更进一步【session技术】就应运而生了。session不仅保存请求过程产生的信息，还因其存放与服务端用户不能随意修改，保证了信息的安全性。
    1.2 session是一种控制管理前后台请求访问过程中产生的一些通用信息的技术。例如，a页面会产生用户信息，b页面需要获得用户名字，c页面需要获取用户年龄
        如果有session的介入，就可以将访问a页面时的用户信息存入【session指定的容器】中，b,c页面可以从session容器中获得需要的信息
    1.3 session容器：是指用来存储会话中产生的相关信息的地方。
        最初这些会话信息是存放在网络服务器中（例如：tomcat），会话信息只能在当前tomcat中流转。
        但是，随着互联网技术的发展和微服务架构被广泛使用后，每个微服务都独享资源，那么会话就需要在这个微服务集中共享。因此，可以将某些具有存储功能的中间当作session容器
        例如：redis，mongodb，sql数据库
    1.4 session是怎么生成的？
        session是有服务端控制的会话技术，因此session也是来自于后端
        session产生的流程：客户端发起请求 访问 httpServletReqeust接口并调用其get.session(ture)方法，
        如果是客户端的第一次请求,那么就会为本次会话创建一个session信息，可以用来存放客户端与服务端交互过程中需要用到的相关信息，并且生成一个sessionId返回给客户端，sessionid是客户端维护与服务端会话的重要唯一标示
        sessionId -> session
        如果客户端已经访问过服务端，那么就通过sessionId获取已存在的session信息
        session完全由服务端控制，一个session信息的创建，修改，删除都是在服务端进行。因此假定一个浏览器关闭了，是不会删除session信息。除非打开另一个浏览器时，这样就服务器就会将其认定为另一个客户端，sessionId将重新生成
    1.4 spring session:是一套有spring提供的实现session管理的接口
        使用 spring-session-redis来管理session
        1.4.1 引入依赖
        1.4.2 进行配置：redis的连接配置，springsession的属性配置
                     【未解决指定session的redis db】
        1.4.3 配置session的序列化策略：com.f.a.kobe.config.SessionConfig
2、关于redis的序列化策略的问题：
    redis序列化的规则，在数据存取过程中的数据格式的约定。
    在通过redistemplate向redis存取数据时，首先会经过一个raw(xxx)的办法，这个方法的作用就是将被操作的按照指定的方式进行序列化或反序列化
    指定序列化的方式是在redistemplate初始化的时候进行
    redistemplate的初始化有两种方式：1、使用模式的redisTemplate，(springboot自动化配置中提供的方案)
                                  2、使用自定义的redisTemplate，在此时指定key/value的序列化：
        【例如：】
        regionRedisTemplate.setKeySerializer(new StringRedisSerializer()); //key的序列化策略
		regionRedisTemplate.setValueSerializer(new FastJsonRedisSerializer<>(Object.class)); //value 的序列化策略
		
		// 设置hash key & value的序列化策略
		regionRedisTemplate.setHashKeySerializer(new StringRedisSerializer());
		regionRedisTemplate.setHashValueSerializer(new FastJsonRedisSerializer<>(Object.class));
		
		//regionRedisTemplate.setDefaultSerializer(new FastJsonRedisSerializer<>(Object.class));
		regionRedisTemplate.afterPropertiesSet(); // 这一步是让自定义的序列化策略生效

    【注意：】在redis-cli中对数据进行查询时，也是要遵循存入数据时的序列化策略的
            如果在存数据时，当前的key未使用new StringRedisSerializer()策略，那么直接使用get key 是会返回 nil，
            原因就是redis-cli get key的默认是string类型的，但是存入的数据又未被StringRedisSerializer，因此是找不到对应数据类型的key导致

第三周(2019/11/11 - 2019/11/15)
已完成的任务：
1、用户基本信息原子服务开发
2、用户地址原子服务开发
3、用户体征信息服务开发
4、向redis同步地址信息，从redis中获取地址信息功能开发
5、【关于通过向redis同步地理位置信息的总结】

第三周(2019/11/18 - 2019/11/22)
已完成的任务：
1、利用redis的atomicLong的自增特性来实现生成全局唯一有序id的功能
  1.1 生成全局有序id的原理是利用redis的incr命令，对指定的key进行自增。并拼接也是规则生成。
  1.2 使用spring-data-redis提供的 RedisAtomicLong.incrementAndGet() 进行
  1.3 这个api底层通过execute(connection -> connection.incrBy(rawKey, delta), true) 调用 redis的 incr 命令对当前key进行自增。
  1.4 如果 key 不存在，那么 key 的值会先被初始化为 0 ，然后再执行 INCR 操作。 
  1.5 此层实现原理：
      127.0.0.1:6379> set num 10
      OK
      127.0.0.1:6379> incr num
      (integer) 11
      127.0.0.1:6379> get num    # 数字值在 Redis 中以字符串的形式保存
      "11"
  1.6 java实现过程：利用
      1.6.1 先声明一个基于RedisAtomicLong的计数器，通过RedisAtomicLong counter = new RedisAtomicLong(key, redisTemplate.getConnectionFactory());
            的构造方法配置需要自增的key，redisconnection，以及过期时间
      1.6.2 key是指计数器的名称(counter_name),如果不同的计数器需要不同的计数器来计算序列号，就各自声明即可。
      1.6.3 【注意】：在声明时不要指定初始值，因为如果一旦指定，每当项目重写启动时，就会将这个key重设为初始值
      1.6.4 key的过期时间的配置：通过counter.expireAt(todayEnd.getTime());配置这个计数器的归零的时刻
            例如 todayEnd.getTime() = 每日的23:59:59:999 就是表示在这个时刻，将计数器key清空重新开始计数。
      1.6.5 【此方法不适用于有[redis集群]的生成全局唯一有序id的场景】
2、通过 WebMvcConfigurationSupport 进行mvc的配置，并结合自定义参数解析器HandlerMethodArgumentResolver实现从session获取请求者信息并转换为当前的请求对象userAgent
   实现步骤如下：
   2.1 new class WeConfig extends WebMvcConfigurationSupport 【 WebMvcConfigurationSupport 新版本springboot推荐使用 】
   2.2 WebMvcConfigurationSupport 已经实现了springmvc功能的默认实现，WeConfig类继承并重写WebMvcConfigurationSupport中的部分方法，是为了配置适用与本项目的个性化功能
   2.3 HandlerMethodArgumentResolver 是一个参数解析器，这个参数解析器【必须】要配合拦截器共同使用，其作用是从拦截的请求中获取一些参数，
       并将这些参数进行包装并改写成为一个ctrl的入参，例如：@PathVariable 注解
       2.3.1 自定义的参数解析器，例： UserAgentResolver implements HandlerMethodArgumentResolver 接口
       2.3.2 重写 supportsParameter 方法，在其中【定义这个参数解析要匹配的参数类型】，例如 UserAgentResolver指定解析UserAgent.class 
             一旦在被拦截的请求路径中springmvc发现需要用到UserAgent类型的参数是，就会进入这个UserAgentResolver进行解析；
       2.3.3 重写 resolveArgument 方法，在这个方法中从被拦截的请求中获取需要的信息，然后经过处理，转换成supportsParameter 中定义的数据类型，
             NativeWebRequest(webRequest)类是获取请求信息的入口，webRequest.getAttribute(ATTR_NAME, RequestAttributes.SCOPE_SESSION);
             RequestAttributes.SCOPE_SESSION -> 从session中获取数据
             RequestAttributes.SCOPE_REQUEST -> 从request中获取数据
             ATTR_NAME -> 指定需要被获取的属性
   2.4 配置拦截器 UserSessionInterceptor implements HandlerInterceptor 
        preHandle ——> 请求已经拦截，但是在执行处理方法之前 -》 例如：拦截请求中是否具备用户的认证信息
        postHandle ——> 请求逻辑已经执行，但是在渲染页面之前 -》 例如：对响应数据进行格式化之后再渲染到页面上
        afterCompletion ——> 请求已经执行结束后 -》 例如：执行完成后记录日志
       自定义的拦击器可以根据需求选择性的重写这三个方法，也可以只是实现不重写，作为一项显式声明的拦截策略存在于WebMvcConfigurationSupport中，配合其他组件来使用
   2.5 开始配置WebMvcConfigurationSupport 
        重写 public void addInterceptors(InterceptorRegistry registry)  方法： 针对某个拦截器指定需要执行的匹配路径。
            使用registry.addInterceptor(自定义拦截器类).addPathPatterns(拦截路径[支持通配符])
                                                    .excludePathPatterns(需要排除的拦截路径[支持通配符])
        重写 public void addArgumentResolvers(List<HandlerMethodArgumentResolver> argumentResolvers) 配置请求数据解析器
            使用 argumentResolvers.add(new UserAgentResolver()); 将自定义的数据解析器加入到springmvc配置中
        重写 public void configureMessageConverters(List<HttpMessageConverter<?>> converters) 配置消息的转换器，一般是为了解决中文乱码或者请求内容支持的相关问题
            converters.add(new StringHttpMessageConverter(Charset.forName("UTF-8"))); // 解决中文乱码问题
		    converters.add(new MappingJackson2HttpMessageConverter()); // 解决 application/json；charset=utf-8 unsupported mediaType 的问题

    【重点总结：HttpMessageConverter】
     springmvc框架是一套mvc框架，其作用是将请求、响应和页面渲染解耦。其最核心的组件是 DispatcherServlet类，有这个类提供请求拦截和跳转控制以及页面渲染的工作。
     不过DispatcherServlet的执行也是需要加载一些相关资源的，这个资源的配置在WebMvcConfigurationSupport中进行。
     WebMvcConfigurationSupport中的getMessageConverters()方法是用来配置本项目中需要的消息解析器，其中有个重要的判断逻辑：
        如果没有自定义的HttpMessageConverters就进行默认加载，其中加载的new StringHttpMessageConverter()的默认字符集是ISO-8859-1
       【反之则只加载用户自定义的HttpMessageConverters】：因此就出现了如果在拦截器中重写StringHttpMessageConverter的charset = utf-8 后，就没办法拦截@Requestbody注解的请求（content-type:application/json）
        此时使用重写extendMessageConverters方法，用来对在默认加载的HttpMessageConverters进行扩展，而不是清空默认列表
     WebMvcConfigurationSupport 是在项目启动是就会根据默认/自定义配置加载到springIOC中，提供了解析各种content-type的支持，等到DispatcherServlet开始执行请求响应流程时提供支持；
     
     请求和响应在被DispathcheServlet处理
        执行doDispath方法
            调用 -> HandlerAdapter ha = this.getHandlerAdapter(mappedHandler.getHandler()); // 获得处理器
            当前处理器 执行 mv = ha.handle(processedRequest, response, mappedHandler.getHandler());  //正式处理请求和响应的结果
            /*
             *在正式过程中会将请求包装成 HttpInputMessage 传入AbstractMessageConverterMethodArgumentResolver.readWithMessageConverters()方法中
             *判断当前的请求类型是否能被来自于WebMvcConfigurationSupport中的List<HttpMessageConverter<?>>列表中的解析器某一个read 
             */
             /*
             *在系统处理完成后返回数据会被包装成 HttpOutputMessage 传入 AbstractMessageConverterMethodProcessor.writeWithMessageConverters()方法中
             *判断当前的请求类型是否能被来自于WebMvcConfigurationSupport中的List<HttpMessageConverter<?>>列表中的解析器某一个write
             */
             最后调用invokeHandlerMethod方法，完成请求的受理
3、 梳理并完成用户登陆，注册，手机号绑定的功能

第四周(2019/11/15 - 2019/11/29)
已完成的任务：
1、完成用户地址信息维护功能
2、完成用户体征信息结构化数据，构建业务视图并同步到mongodb中
3、使用CountDownLatch类，在单元测试中使用多线程来方式来测试，通过CountDownLatch也是一种让主线程等待子线程完成后结束的方法，Thread.join() 方式类似
   有点在于，使用CountDownLatch是异步进行的，子线程不需要排队，非常适用于模拟高并发请求。
   用法 ：主线程 声明 CountDownLatch countDownLatch = new CountDownLatch(count); // count 是线程计数器，一般与需要开启的线程数量一致
               子线程： forkThread(CountDownLatch countDownLatch)
                            --> run(){
                                // do xxxx
                                countDownLatch.countDown();  // 子线程run方法中业务执行完成后 调用 countDownLatch.countDown(); 扣减计数器
                            }
        主线程： countDownLatch.await();   //主线程等待子线程进行完毕
4、关于mysql-connector-java 8.0X 版本的jdbc连接配置的正确写法
    driverClassName: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://118.190.53.214:3386/fa_kobe?useUnicode=true&characterEncoding=utf-8&useSSL=false&serverTimezone=GMT%2B8

5、开发chart数据视图功能

第五周(2019/12/01 - 2019/12/06)
1、SpringSession实现会话信息共享的机制的分析
  1.1 SpringSession 是一个用于分布式系统共享会话信息的一种技术方案，可以解决微服务系统架构中用户请求会话共享的难题
  1.2 SpringSession 对系统而言，是一个代替传统由网络服务器存放和维护会话管理的机制，SpringSession中提供了一个用来存储和维护会话信息的接口，SessionRepository
      这个接口提供了不同的数据存放方案，例如：RedisSessionRepository,MongoSessionRepository 等
  1.3 加载过程：
    web项目启动时的web组件加载过程：context-param -> listener -> filter -> servlet
    会话管理器是在一个项目中只需要被配置一次，然后全局使用，所以这是一个过滤器filter的配置
    >>>>>> filter注册流程
    由class SessionRepositoryFilter 选中需要加载的SessionRepository
    由class s SessionRepositoryFilterConfiguration.FilterRegistrationBean 将被选中的SessionRepository的实现注册到filter中
    那么SessionRepositoryFilter 又是如何选中 SessionRepository的具体实现方案的呢？
    >>>>>> sessionRepository选中流程
    【重点：@EnableRedisHttpSession】这个注解是使用默认的RedisSessionRepository的开关,如果不使用的配置，就需要在配置文件中声明
    @EnableRedisHttpSession -> @Import(RedisHttpSessionConfiguration.class) 
    - > RedisHttpSessionConfiguration implements SpringHttpSessionConfiguration,BeanClassLoaderAware
                |—— @Override BeanClassLoaderAware.setBeanClassLoader () // 在项目启动时将当前类初始化成一个bean
                |—— @Bean public RedisIndexedSessionRepository sessionRepository() 将RedisIndexedSessionRepository初始化为sessionRepository
    这样以来，就会在启动的时候，将RedisIndexedSessionRepository作为选中的sessionRepository方案了
    【注意：**sessionRepository只是作为session的存储和维护的方案，也就是说，这只是springsession中的一个部分】
    >>>>>>> 获取session的过程
    所有的请求请求进入系统的过程中，需要先通过filter，通过过滤器链filterChain.doFilter(request, response); 执行dofilter方法，将请求交给系统中已经配置了过滤器
    如果配置SessionRepositoryFilter 那么，将请求交给 SessionRepositoryFilter.doFilterInternal()方法
    在这个方法中会通过 new SessionRepositoryRequestWrapper(request, response)，对当前的请求进行一次在此包装
    这个包装中最重要的步骤
        >>> 解析sessionId : httpSessionIdResolver.resolveSessionIds(this);   //this 是当前请求
        >>> 查询sessionId : sessionRepository.findById(sessionId)
        >>> 设置session : setCurrentSession(currentSession); 将SessionRepository存放的session信息，设置到服务端session中
        setCurrentSession方法 返回 HttpSessionWrapper 对象，这个对象继承HttpSessionAdapter，HttpSessionAdapter又实现了HttpSession的方法
        因此，如果需要获得session信息时，直接使用HttpServletRequest.getSession().getAttribute(“attr_name”);就可以获得到了
    【****这样做的好处是：无论session采用什么方案进行存储，但是对于获取session的方法与普通做法都没有任何区别****】

2、使用header来代替cookie 实现session信息的共享，使用如下方案，替换默认配置即可
    @Bean
	public HttpSessionIdResolver httpSessionIdResolver() {
		return HeaderHttpSessionIdResolver.xAuthToken(); 
	}
    HttpSessionIdResolver：是一个session解析器接口
        ｜—— CookieHttpSessionIdResolver // 默认实现，从cookie中获取session信息
        ｜—— HeaderHttpSessionIdResolver // 从header中获取session信息 

3、搭建kidd项目作为网关，实现在微服务架构中session的共享
4、改造kobe项目，将其他项目中可能会用到组件进行抽离
    >>>> 踏坑：
        在进行公用组件抽离之时，希望将用于将userAgent的转换器配置独立成一个 UserAgentConfig.class extends WebMvcConfigurationSupport  并加上注解@Configure。
        预期是：UserAgentConfig 仅作为配置请求参数中userAgent的转换器配置，可与其他的WebMvcConfigurationSupport共同使用。
        实际上：一个项目中只能存在一个WebMvcConfigurationSupport
        最后方案：只是将userAgentResolver抽离出来，并将其声明为一个bean，在各自项目中的WebMvcConfigurationSupport中加入到参数解析器列表
        详情见：/kobe/src/main/java/com/f/a/kobe/config/webconfig/WebConfig.java
5、商场系统规划：
    5.1 商城系统的主要功能：
            用户界面：查看商品、选购商品、评价商品、加入购物车、生成订单、申请退换、商品比对;
            商户界面：申请入驻，申请撤离、上报商品、维护商品、受理退换、评价回复、商户店铺维护;
            平台界面：系统项维护(商户类型[自营、三方商户]，商品大类别[外卖、运动装备、食品补计、生活用品、其他])、商户管理;
    5.2 商城系统主要的业务流程：
            5.2.1 平台业务：审批商户申请与撤离流程
            5.2.2 商户业务：商户申请入驻与撤离流程
            5.2.3 用户业务：购买商品流程，退货换货流程
    5.3 商城系统中主要的模型：
            5.3.1 商户相关模型：商户基本信息、商户详情信息、商户类型
            5.3.2 商品相关模型：商品基本信息、商品详情，商品评价、商品分类
            5.3.3 订单相关模型：购物车、订单基本信息、订单详情、退换申请
    5.4 商城系统需要实践的主要技术：
            5.4.1 垂直分表技术的运用
            5.4.2 水平分表技术的实践
                    ｜—— 水平分表的聚合查询
                    ｜—— 自动化建表技术的实践
            5.4.3 读写分离技术实践
                    ｜—— mysql proxy
                    ｜—— mycat
                    ｜—— sharding
            5.4.4 通过redis来实现热销产品的排名
            5.4.5 消息处理与事物的一致性的处理
6、商城系统搭建
第六周(2019/12/09 - 2019/12/13)
    1、集成mybatis-plus：是mybatis的加强版
        6.1.1 通过代码配置的方式灵活的配置逆向生成的代码内容
        6.1.2 提供了多种注解，来对表或者字段提供加强的功能：源代码位置：package com.baomidou.mybatisplus.annotation
                例如：@TableName("`Order`") -> 转译表名为关键字的情况
                     @TableField("`status`") -> 转译字段名为关键字的情况
                     @TableId(value = "id", type = IdType.AUTO) -> 设置主键的情况
        6.1.3 mybatis-plus最大的特点是可以自动生成service的代码，进一步释放编写底层代码，进行复杂条件判断的工作量。
        6.1.4 如果不能满足要求，可以在xml加入自定义的sql，或者在service中对已有的方法进行重写
        6.1.5 关于LocalDateTime前端显示的问题：在被声明为LocalDateTime类型的字段上加上  
            @JsonFormat(pattern = "yyyy-MM-dd HH:mm:ss") // 格式化注解
            @JsonSerialize(using = LocalDateTimeSerializer.class) // 序列化注解
    2、zuul+fegin+redisSessionRepository 方式下：请求转发过程中的用户信息的共享
        请求通过zuul转发时，会将sessionId转发到对应的服务，然后后置的服务通过sessionId到对应sessionRepository中去获取session中的信息

    3、订单数据采用垂直分表的方案存储数据，将订单的可变数据与不可变数据分别写入order/order_detail中
    4、通过mysql的事件调度器（Event Scheduler）实现自动化建表
        创建过程如下：
        -- 查看当前数据库的事件
        SHOW VARIABLES LIKE 'event_scheduler';
        -- 开启事件调度器 on/off 或 （1/0）
        SET GLOBAL event_scheduler = on;

        -- 查看事件调度器的状态
        SELECT @@event_scheduler;

        -- 删除存储过程
        DROP PROCEDURE IF EXISTS order_create_by_day;

        -- 添加; 的转义
        DELIMITER ;;
        create procedure order_create_by_day()
        begin
        DECLARE tblname VARCHAR(32);
        set tblname =  CONCAT('order_', date_format(now(),'%Y%m%d'));
        set @sql_t = concat("create table ",tblname,'(
               `id` bigint(16) NOT NULL AUTO_INCREMENT COMMENT \'主键\',
          	 `order_id` varchar(32) NOT NULL COMMENT \'订单编号\',
          	 `user_account` varchar(32) NOT NULL COMMENT \'用户编号\',
          	 `goods_id` varchar(32) NOT NULL COMMENT \'商品编号\',
          	 `category` varchar(16) NOT NULL COMMENT \'商品类型\',
          	 `num` int NOT NULL COMMENT \'商品数量\',
          	 `price` varchar(32) DEFAULT NULL COMMENT \'应付总价\',
          	 `discount_price` varchar(32) DEFAULT NULL COMMENT \'优惠价格\',
		 `settlement_price` varchar(32) DEFAULT NULL COMMENT \'结算价格\',
          	 `merchant_id` varchar(32) DEFAULT NULL COMMENT \'商户号\',
		 `status` varchar(8) NOT NULL COMMENT \'订单状态\',
		 `order_time` datetime DEFAULT NULL COMMENT \'订单创建时间\',
          	 `finish_time` datetime DEFAULT NULL COMMENT \'订单完成时间\',
          	 `cdt` datetime DEFAULT NULL COMMENT \'创建时间\',
          	 `mdt` datetime DEFAULT NULL COMMENT \'修改时间\',
          PRIMARY KEY (`id`)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4');
        prepare sql_t from @sql_t;
        execute sql_t;
        end;
	
        -- 定义执行计划
	 DROP EVENT IF EXISTS table_auto_create_by_day;	
        CREATE EVENT table_auto_create_by_day
        ON SCHEDULE every 1 day 
        starts timestamp(current_date,'09:32:00')
        on completion PRESERVE
        DO  call order_create_by_day();

    5、通过mybatis-plus实现动态表名的机制：
        当系统中某些表中的数据增量很大，就需要对这个表进行水平切分。而通过程序进行数据的读写操作，就需要克服动态表名带来的问题，实现原理是通过拦截器，在进行prestatement操作是对sql进行改写。
        通过mybatis-plus可以通过较少的配置来解决这个问题：
        解决步骤如下：
        5.1 声明一个配置类，用于进行mybatis的配置
        5.2 在这个配置类中，声明一个 PaginationInterceptor 的@Bean
        5.3 在这个bean中进行DynamicTableNameParser.setTableNameHandlerMap()中的ITableNameHandler的实现,最后加入到PaginationInterceptor的sqlparse中
        @Bean
        public PaginationInterceptor paginationInterceptor() {
		    logger.info(">>>>>加载动态订单表表名的策略: order_yyyyMMdd");
            PaginationInterceptor paginationInterceptor = new PaginationInterceptor();
            DynamicTableNameParser orderdynamicParser = new DynamicTableNameParser();
            //设置动态表名的策略,注意表名如果是sql关键字要加上``
            orderdynamicParser.setTableNameHandlerMap(new HashMap<String, ITableNameHandler>(2) {{
              put("`order`", new ITableNameHandler() {
			    	@Override
				    public String dynamicTableName(MetaObject metaObject, String sql, String tableName) {
				    	String dynamicDate = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd"));
	                   return TABLE_ORDER+"_"+ dynamicDate;
				    }
                });
            }});
            paginationInterceptor.setSqlParserList(Collections.singletonList(orderdynamicParser));
            return paginationInterceptor;
        }
        PaginationInterceptor 是mybatis的拦截器，通过这个拦截器可以完成分页配置，sql改写等操作；
        paginationInterceptor.setSqlParserList 是添加一个动态sql解析器
        动态sql的编译和执行过程的入口是DynamicTableNameParser.parser方法，在这个方法中会获取需要改写的表名和对应改写规则
        【这个规则来源于对ITableNameHandler接口dynamicTableName方法的实现】>>>【最后执行ITableNameHandler接口中的process方法来改写sql】
        但是如果面临需要进行数据进行跨表的聚合查询时候，mybatis-plus就无能为力了。。。需要引入其他框架来解决这个问题。

        6、灵活的使用mybatis中提供的@Select等注解，实现jdbc操作，可以有效的降低编写xml的复杂度


2020-04-06 2020年从一开始就注定不会平凡，经历了novel coronavirus的劫难我们，在提心吊胆日子里掰着指头数着过，最幸福的事情莫过于新的一天还能与亲人朋友互道一声早安，午安和晚安。
           终于，阳光穿透了厚厚的云层，铺洒到城市的每一个角落，我们也陪着自己的城市，度过了她最艰难的日子。
           虽然错过了武汉的樱花期，但是好在马路的喇叭声逐渐多起来，早点摊前排队的人慢慢变多了。我成长的地方，我爱的城市正在有条不紊的复苏中。
           体会过了苦难，才切肤的体会到平凡与健康才是最难得的福祉。
           70多天后再次回到办公室，工作的感觉并没有被消毒水味道所稀释，也希望自己能够快点的进入到工作的状态和节奏中去。
           希望从今年正式开始的2020年，不要在继续差下去了，记得珍惜身边遇到过的每个人，珍惜指缝里划过的每一刻时光，毕竟没有任何一个昨天或者明天比今天更重要。

    webSocket技术积累
    webSocket是浏览器内置对象，使用的前置条件是客户端浏览器支持websocket对象。webSocket协议本身是跟http没有任何关系的，但是握手过程借助了http协议
    WebSocket是HTML5 开始提供的一种在单个 TCP 连接上进行全双工通讯的协议。通过WebSocket协议可以建立前后端的长连接，浏览器和服务器只需要完成一次握手，两者之间就直接可以创建持久性的连接，并进行双向数据传输。
    使用WebSocket建立长连接的交互方式，相比使用ajax+轮询的方式而言，具有更好的及时性并且很大程度上减少频繁建立连接的资源消耗。
    同时，在海量并发及客户端与服务器交互负载流量大的情况下，极大的节省了网络带宽资源的消耗，有明显的性能优势，且客户端发送和接受消息是在同一个持久连接上发起，实时性优势明显。
    
    websocket的请求头和响应头中的特殊参数：
        request-header：{
                            connection:upgrade;     //本次请求需要被升级
                            upgrade:websocket;      //本次请求升级为websocket协议
                            sec-websocket-key: xxxxxxx  // 客户端生成一组16位的base64编码的随机数，用来标识这个请求
                        }
        response-header：{
                            sec-websocket-accept: xxxxxxx  // 响应时经过固定算法加密后的响应标识，必须与sec-websocket-key中的值一致，才能被客户端接受
                        }


    如果客户端刷新页面（例如用户离开后再次进入，websocket将重新建立连接）
    
    
    在页面创建和使用websocket功能：
    1、客户端创建 websocket ：var Socket = new WebSocket(url, [protocol] );
    2、客户端监听 websocket 事件：
        Socket.onopen：监听建立连接事件
        Socket.onmessage：监听服务端返回成功的信息，返回的信息会被封装在一个event对象中 Socket.onmessage = function WSonMessage(event) - > event.data 获取服务端返回的数据 WebSocketSession.sendMessage(textMessage);
        Socket.onerror：监听服务端返回异常事件
        Socket.onclose：监听断开连接事件
    
    服务端建立连接
    1、服务端要实现HandshakeInterceptor接口

    2、实现WebSocketConfigurer接口重写registerWebSocketHandlers：将ws的拦截地址注入到服务端中【代码示例如下：】
        @Override
	    public void registerWebSocketHandlers(WebSocketHandlerRegistry registry) {
		    registry.addHandler(new ChatMessageHandler(), "/webSocket/{INFO}").setAllowedOrigins("*") // path地址匹配页面上new WebSocket(url)-> url（如果项目地址有请求上下文，在java端也不需要写）
                .addInterceptors(new WebSocketHandlerShakeImpl());
	    }
        【注意： 通过 new xxHandler() 注册处理器时，被注册的处理器并不是一个bean！！！！因此也无法在注册器类中使用任何依赖关系！！！！】
        ======》》》》》改进方式：将 new xxxHandler() 主动声明成一个bean
        @Bean
        public xxxHandler  xxxHandlerBean (){
            return new xxxHandler（）
        }
         @Override
	    public void registerWebSocketHandlers(WebSocketHandlerRegistry registry) {
		    registry.addHandler(xxxHandlerBean(), "/webSocket/{INFO}").setAllowedOrigins("*") ）
                .addInterceptors(new WebSocketHandlerShakeImpl());
	    }


    3、实现自己的WebSocketHandler处理器，重写对应的方法：
        afterConnectionEstablished：连接建立成功执行的方法
        handleMessage：消息处理方法 ——> WebSocketSession对象维护客户端和服务端的连接信息，提供向客户端发送信息的方法 / WebSocketMessage对象保存客户端的信息内容
        handleTransportError：消息处理失败的方法
        afterConnectionClosed：连接关闭后的方法
        supportsPartialMessages：是否要对一个大信息分割成若干个小信息


    4、如何维护客户端与服务端的回话映射关系：
        构建一个map结构-> map<roomid,map<currentUserId,websocketSession>>
        第一层map 维护的是 房间号<->当前房间号中所有的聊天记录的关系
        第二层map 维护的是 房间中的人<->他自己的会话的关系

    服务端/客户端最大连接数和最大并发数的处理
    客户端的最大连接数取决于客户端自己的限制，例如：chrome max = 256 / safari max = 1400+
    服务端的最大连接数，取决于部署服务器的配置 max_thread 


【关于too many connections error的解决方案：https://www.jianshu.com/p/fc40067c4dc9 /
     springboot 数据库连接池的配置 https://www.cnblogs.com/hugb/p/10493101.html】

2020-06-18 jenkins 学习
持续集成：让代码可以持续的获得新的功能
持续交付：让工程能够持续的面向用户交付

jenkins的执行过程：
    开发人员：提交代码 -> 提交/推送到 代码仓库 -> [触发钩子程序] -> 通知Jenkins 
    jenkins: 调用git/svn插件 -> 获取代码 -> 调用 maven插件 进行构建 -> 调用 deploy to web container 插件 -> 部署与发布

jenkins的安装：
    1、下载jenkins 程序
    2、将其部署到tomcat中进行启动，注意：修改tomcat的 uriencoding = "utf-8"
    3、启动应用，访问并根据页面提示流程进行初始化配置，选择推荐安装的插件
    4、（git）需要安装git程序

jenkins的配置：
    0、配置用户发布项目的tomcat -> 需要配置使用tomcat的用户名密码，在部署过程中需要使用
    1、在当前环境中安装maven和JDK
    2、安装插件：【deploy container to web】：使jenkins可以将war部署到指定位置的容器中
               【git-plugin 或 svn-plugin 】：连接代码仓库
               【maven-Integration】：构建maven工程
               【publish over ssh 】：执行远程部署的插件 -> 在【系统配置】菜单下配置对 ssh 进行配置
    4、在github上配置webhook

jenkins的使用：
    1、新建任务： 选择构建maven项目（前置条件：在jenkins中安装了maven插件）
    2、配置： 
        【通用】中的信息，这些信息是对工程（job）的描述
        【源码管理】：选择git，并填写需要被构建的git仓库与指定分支
        【构建触发器】：定义在自动触发构建操作的条件
        【构建环境】：构建过程中的环境配置：Delete workspace before build starts：在构建之前清空工作空间
                                      Abort the build if it's stuck：如果构建出现问题则终止构建
                                      Add timestamps to the Console Output：给控制台输出增加时间戳
                                      Use secret text(s) or file(s)：使用加密文件或者文本
        【构建】：正式进入打包环节
                【pre steps】：构建之前需要执行的任务
                【build】：构建配置:构建完成后包地址默认：/var/lib/jenkins/workspace/${job_name}/target/ 
                          可以在高级选项中自定义包存放地址：例如：/home/jenkins_archive
                【post steps】：构建完成需要指定的任务
                            例如：1、构建完成后执行脚本，部署和启动本地项目
                                 2、通过SSH插件，将包文件推送到指定远程服务器并执行部署与启动任务

构建过程中的报错处理:
    1、RPC failed; curl 18 transfer closed with outstanding read data remaining
        解决方案：git config http.postBuffer 524288000
        错误原因：需要下载的代码量太多，会导致超时等问题，所以就要调大postBuffer的配置
        


    
        


2020/06/21 - 2020/06/24 mysql集训
1、mysql是单进程，多线程的机制
    1.1 客户端与mysql服务端的连接是用过在服务端开启一个长连接的线程来维护的
        查看命令： show global status like '%Thread%' 
                 show global variables like 'wait_timeout' -> 非交互式的连接超时时间，例如JDBC
                 show global variables like 'interactive_timeout' -> 交互式的连接超时时间，例如navicat
        对于超时的连接，服务器将自动回收
    1.2 mysql的最大连接数：理论上支持的并发连接是10万个，默认是151个 
    1.3 配置的级别：
            session：当前会话 -> 影响当前窗口中运行的命令的执行结果 show  variables like 'wait_timeout'
            global ：全局-> 影响当前数据库中所有命令的执行结果 show global variables like 'wait_timeout' 
    1.4 修改配置：
            动态修改：set autocommit=on;
            永久修改：修改 my.cnf 文件后重新启动

    1.5 数据库自带的缓存：5.7 版本是默认关闭的，在8.0版本就直接将其删除掉了
            原因：自带的缓存有些鸡肋，例如：A、如果更新了查询中的一条数据，那么所有的之前查询并缓存到内存中的结果都将失效，
                                       B、mysql本身是不区分大小写的，但是如果前后两次查询语句的大小写发送了变化，缓存也将失效
   
    Server 层：解析、计算、排序
    1.6 一条查询语句的执行过程：
        0、查询缓存（query_cache）：虽然数据库默认不开启 
        1、语法解析(parser)：解析语法和结构 
        2、预处理(pre processor)：解析权限和语义
        3、优化器处理(optimizer)：分析查询语句的执行路径，并选择最佳执行路径
        4、生成执行计划(explain)：基于【cost】生成执行计划，但可能并不是最佳方案（因此需要手动优化）
        5、执行引擎（execution engine）：通过统一API调用待执行语句的数据引擎，返回客户端

    1.6.1 一条更新语句的执行流程：insert/update/delete
        0、命令解析
        1、从buffer pool获取数据
            1.1、如果没有就从disk中获取数据，并返回到buffer pool中，并产生脏页（未提交的内容）
            1.2、同时在server中追加binlog
        2、在buffer pool中执行更动作
            2.1、更新redo.log
            2.2、记录undo.log
        3、后台线程刷新buffer pool中的脏页到disk中（执行提交动作）

    1.7 binlog：执行日志记录（可追加的文件） - > 主要用来：数据恢复/主从复制
        一条更新语句的执行过程：InnoDB引擎


        
    Storage 层：获取数据
    1.8 mysql的集中常见的数据库引擎 storage enigine
        InnoDB: 支持事务，支持分布式事务的两段式提交，行锁定
        MyISAM: 查询比较快，但不支持事务
        Memory：数据存在内存中，查询快，但无法持久化
    1.9 数据引擎【InnoDB】与磁盘之间，有一块存在于内存中的区域：buffer pool，这个区域缓存了还没提交的数据（脏页）
        这是buffer pool 的存在，是的数据从server层，到disk之间有了一个高速的缓存桥梁，读写操作先通过buffer pool，再通过后台线程提交到disk中
        redo.log：重做日志：文件大小是固定的，只能作为崩溃恢复
            这个文件存在于磁盘上，用来记录buffer pool中的数据，防止数据在buffer pool 提交到 disk之前出现了系统崩溃导致的数据就是的问题
            【redo.log 是一个顺序I/O模型，生成的文件在磁盘上是连续位置的，避免了随机I/O的寻址蛮的问题】
        undo.log 回滚日志

2、数据库的索引：索引不要轻易用，因为索引会占用更多的存储空间，并且建立和更新索引时会很费时间
    2.1、索引到底是什么：索引可以用在众多的存储系统中，相当于存储系统中的目录
         数据库的索引是一个排序的数据结构，以协助快速查询、更新数据库中的数据。
    2.2 InnoDB中索引的类型：在磁盘中以 k/v的方式存在
        normal：
        联合索引：
        unique：
        主键索引：unique+not NULL
        FullText：全文匹配 -> 解决大字段中，模糊查询索引失效的问题（对非英文的查询支持不好）

    2.3 二叉查找树（BST）：
    2.4 平衡二叉树（AVL BST):解决BST在极端情况下，不平衡的导致的无法提高查询效率的问题
                特点：左右子树深度差绝对值不能超过1
    2.5 多路平衡查找树 B-Tree:相比二叉树而言，b-tree的层级更少，在通过索引查询时需要进行I/O的次数更少，效率更好【关键字数量 = degree-1】
            node存储的索引 = 索引内容+degree+索引所在的地址信息
    2.6 加强版的多路平衡查找树：B+tree：相比b-tree，高度更低了
             node存储的索引 = 索引内容+degree，区别于B-tree：索引的地址信息是在叶子节点上，而叶子节点又是一个双向链表，链表上有指针
    2.7 hash 索引：时间复杂度更低，查询效率高。但是：不能避免k重复时，出现了hash冲突导致的性能下降，例如，在性别字段上使用hash

    2.8 索引在存储引擎上的实现：
    MyISAM :索引和数据分开存储，索引存在.MYI文件中,索引和数据的映射关系存在.MYD文件上
    INNODB：数据和索引存在一个文件中，【索引组织表】数据存在索引上，索引决定数据 -> 也就是说数据都挂在b+tree的节点上
           【聚集索引】：决定数据的屋里存放位置：例如：主键索引
            如果没有主键怎么办：1、找到第一个唯一索引且不为空的字段作为聚集索引
                             2、如果没有任何索引时，会创建一个隐藏的ROW ID作为聚集索引

    索引怎么用：索引用在查询条件上
    通过explain 查看 -> possibleKey / key 表示可能用到的索引，已经用到的索引
        1、联合索引的最左匹配原则，
        2、覆盖索引：这是一种索引的使用情况，并不是索引的类型，在查询的内容就是索引字段的情况下触发，在这个情况下，数据库不会进行回表操作
                  通过explain查询：extra字段显示：Using index  
        3、字段重复值高（区分度低）的字段不用索引
        4、索引存储引擎下推：在查询条件中，如果存在 like '%xx' add inx(索引字段) = value ，可能会触发让下推到存储引擎中去执行索引
        5、在 != / not NULL / <> 是不会使用索引的

    索引是否生效取决于 优化器基于 cost的成本来决定是否使用索引

3、mysql的事务控制（Innodb引擎）

    事务四特性：ACID
    事务的开始和关闭：
        自动开启：查看自动开启状态：show variables like 'autocommit';
                set session/global autocommit = on / off;
        手动开启：begin 
        执行DML：update / insert / delete
        关闭事务：commit/rollback
    数据库记录事务的表：INNODB_TRX

    并发问题的三种原因：
        P1:脏读：A-> select 操作  B - > update 未 commit  A-> select  >>> 得到的结果来自于 buffer pool 中还未刷脏的数据 ，所以称之为 脏读
        P2:不可重复度：A -> select 操作 B - > update  commit  A-> select  >>> 得到的是 刷脏后的数据，前后两次查询的结果不相同，所以称之为：不可重复读【发生在update/delete】
        P3:幻读：A -> select 范围查询操作 B -> insert commit  A -> select 范围查询操作 >>> 得到的结果比第一次读的记录要多，所以称之为：幻读 【发生在插入操作】

    解决办法：事务的隔离级别【事务的隔离级别，是有专家定义的解决并发问题的几个标准，又数据库厂商负责实现】
    read uncommited：读未提交：P1、P2、P3 都可能出现
    read commited：读已提交：P1、不会出现，P2、P3 都可能出现
    repeatable read：可重复度：P1、P2不会出现，P3 可能出现
    serializable：串行：P1、P2、P3 都不可能出现

    数据库默认的事务隔离级别：repeatable read
        注意：在innodb中是不会出现 幻读订单情况，所以repeatable read 已经是最佳方案

    如何解决，读数据时，不被其他修改操作所干扰？（如何解决 读/写的并发问题）
        【在mysql Innodb中每一行数据都有三个隐藏字段：
            DB_ROW_ID:在没有主键索引时候的默认聚集索引字段
            DB_TRX:ID:事务id，每执行一次事务，就会自动递增（创建版本号）
            DB_ROLL_PTR:执行一个回滚的地址 -> undo.log 
        】

        1、基于表的锁（LBCC）：
        2、生成快照：（MVCC）:

    Innodb中的锁：
        加锁的方式：自动/手动
        释放锁的方式：事务结束
        行级别：
        共享锁：读锁 >>> 不同的查询线程，可以共享读锁（select * from tablename lock in share mode），持有读锁的线程不会阻塞其他线程
        排它锁：写锁 >>> 执行写操作时，自动加锁/手动加锁（for update ），在持有当前锁时，其他任何操作都不可以进行

        表级别：数据库自动执行的
        意向共享锁/意向排它锁：：在表中的任意一个数据加上了行锁（读/写），都会在表上添加到一个意向标签，表示当前的表中有某条数据正在被锁定，意向标识的本质是解决了，需要扫描整张表去检查是否有行锁存在的效率问题
        
    锁解决的问题是什么：是解决资源竞争的问题，控制请求获得访问指定资源的权限
    到底锁的是什么：锁的是这一行的索引，通过锁定这一行的索引，实现控制这一行资源的能力。其他请求线程，无论以何种方式都无法获得被锁行数据的控制权。
    【注意：在没有索引的表上进行锁操作，会导致锁表的问题:锁表问题的根本在于，虽然表未显式声明任何索引，但会存在一个有一个隐式的聚集索引ROW_ID，而对未有显式所以的表进行加锁操作
    实际上是对每一行row_id进行了加锁，到时整张表都处于排它状态，于是就有了锁表的情况】

    锁的算法：
        record lock 记录锁：锁定行资源，触发条件：通过唯一索引进行等值查询，精确的匹配到指定行 
        gep lock 间隙锁：锁定不存在的资源区间，例如：有ID = 1，ID = 4，ID = 7，ID = 10的四行数据，当希望插入条件为ID = 6 此时将会触发间隙锁导致无法插入成功
                 间隙锁的最大作用就是为了阻塞插入
        next-key lock：临建锁：锁定范围内的资源，触发条件：范围查询，包含记录和区间，锁的范围是：起与左边界相邻的左一位 至 右边界的右一位 的范围，在这个范围类，无法进行写入操作


    SQL优化：
        0、查询mysql服务端的状态：show global status；
           查询当前的线程连接数：show processlist；
           查询存储引擎的状态：show engine innodb status; -> 存放了当前存储引擎的状态，例如后台执行的线程，锁情况等
                            通过 set global innodb_status_output = on 开启标准监控模式
                                 set global innodb_status_output_locks = on 开启锁监控模式 
        1、如何解决 too many connections: 优化配置
            原因：并发量太大  / 慢查询太多，导致连接被占用 
            服务端：
                调整 wait_timeout >>> show variables like 'wait_timeout'; 
                调整 max_connections >>>  show variables like 'max_connections'; 
            客户端：
                采用连接池技术
        2、硬件优化
        3、架构层面优化：
            缓存架构：将查询和不常变化的数据放在缓存中
            集群：1、读写分离 主写/从读
                 2、分库分表
        4、记录慢查询的日志：show variables like 'slow query'; >>> slow_query_log ON / slow_query_log_file 日志文件的位置
            注意：默认为关闭，开启日志会增加资源的消耗，日志文件文件中记录的是 低于系统设定的慢查询标准的执行记录

        5、分析sql 查询慢的原因：explain 查看执行计划
            id : 反应执行查询(多表/联合查询)过程中，表的访问顺序，如果id相同那么访问顺序是从上到下的，如果id不相同，是访问id值大的，再访问小的
            select_type:PRIMARY-主查询/SUBQUERY-子查询/DERIVED-派生表【在联和查询中用于存放中间结果的临时表】
            table:表名
            partitions:分区
            type（重点）:查询类型(性能从高到底)：system > const > eq_ref > ref > range > index > all
                        system：很少见，只有在进行数据库系统信息查询时才会遇到
                        const:常量查询，查询条件是主键，且条件匹配的是一个常量，才会触发 select * from table where id = 1；
                        eq_ref:在进行join查询时，被驱动表使用了 唯一索引 会触发 
                        ref:查询条件存在非唯一索引会触发
                        range:条件为普通索引进行范围扫描时触发，例如 between
                        index:查询普通索引的全部索引数据，例如 ：select tcid(普通索引列) from teacher，此时会扫描整张表的索引数据
                        all: 未用到索引 
            possible_key: 可能会用到的索引字段
            key:实际用到的索引字段（当possible_key 为空时，key已然可能会存在，当查询条件本身就是索引字段时）
            key-len:索引字段的长度 
                    例如：在 utf8mb4编码格式下的varchar字段是一个索引字段时：len = 字段的长度*4 +2 (如果是变长字段) + 1(如果允许为空) 
                         假设 该字段为 varchar(255) - > len = 255*4+2+1 = 1023
            ref:在key列记录的索引中，表查找值所用到的列或常量，常见的有：const，func，null，字段名
            rows: 预估扫描的行数，这个值只是一个参考值，但是，预估扫描的行数越少，效率越高
            filterd: 查询结果在返回之前通过存储引擎层完成过滤的百分比，这个值越高越好
            extra:

        6、表结构、存储引擎的优化
            合理设计字段类型和长度>>> 值相对固定的字段，尽量选用定长字段，节省存储空间和效率
            表拆分
            字段冗余

        7、在特殊场景下，还可以通过业务设计层面来执行功能降级，以保证最核心的算力来执行最重要的最核心的业务
            例如：支付宝在双11的0点-1点 只能查询当天的交易记录，而短时内不支持历史交易记录的查询
                 京东618，采用预收分流的运营方案，让用户从618前一周开始，就能享受最优惠的活动价格，目的也是为了执行业务分流

        
        8、如果一个查询语句执行很慢，怎么去解决？
            1、使用explain 查看执行计划
            2、分析当前sql的执行情况，了解表的执行顺序，访问类型，索引，扫描行数等信息
            3、解决问题的办法：用排除法，通过调整表执行顺序，逐个递减条件来，逐步试验和尝试来定位慢的原因
                比如：是未使用索引，还是关联查询引起，或是order by 引起的
            4、对症下药：
                1) 创建索引或联合索引
                2）改写sql：
                    2.1） 在联合查询是用小表驱动大表
                    2.2） 用join来代替 子查询
                    2.3） not exist 转化为 left join is NULL
                    2.4） or 改成 union
                    2.5） 在不去重的前提下，用 union all 代替 union
                    2.6） 如果有大的偏移，先使用limit 排除，在执行过滤
                    



rxjs：结合了 观察者模式 | 迭代器模式 | 函数式编程 的优点
是一个js 的库，适合编写 非同步，以事件为主的程序，通过 observable sequences 操作数据

Reactive programming: 一种基于异步的数据流的编程
Stream: 一系列的操作
非同步的时序控制：请求 <-> 响应 顺序不一样是，用rxjs 比较容易控制
主要目的：有效管理非同步环境下的事件资料
    非同步：ajax，setTimeout，setInterval，promise
    事 件：各种dom事件，css，html5（Geolocation,websocket,server send event）

核心概念：
observable : 可观察的事件
observer : 观察者
**【observable <->observe 成对出现，observe必须要订阅后observable才会执行】**
subscription： 控制 【observe 订阅->observable】的执行整体，可以用来取消订阅 
operations：运算符 -> 用来处理 从 observable -> observe 的数据重组 【将近100个，常用十几个】
subject: 主题 处理一对多的关系
schedule: 调度器，控制事件并发情况

example：
    rxjs.interval(500)                ->| 创建 observable
        .pipe(                        ->| 创建 管道 进入 stream 操作
            rxjs.operators.take(4)    ->| 使用 rxjs 预算符进行操作
        ).subcirbe(console.log)       ->| 订阅 ->观察者 (观察者需要执行的函数)
                                      ->| 以上步骤，就是创建了一个完整的创建客观察事件 -> 传递信息 -> 观察者 的过程
                                      ->| 如果用 变量 var sub 来接受这段代码，sub就是一个 subscription 对象，在需要的是用可以调用 .unsubcribe方法来停止订阅                                   

分步说明：
创建可观察者
var clicks$ = rxjs.fromEvent(document,'click')
创建观察者
var observer = { next:(x) => console.log(x)}
让观察者观察可观察者,组成订阅对象
var sub$ =clicks$.subcirbe(observer);
取消订阅
sub$.unsubscribe(); 

observer 的三种属性：
    next：正常执行的内容
    complete：执行完成后需要执行的内容
    error：执行发生错误以后，要执行的操作

rxjs.operator ：https://rxjs.dev/guide/operators
pipe(
    take(n), -> 执行到几 : 如果到n，这个函数就结束了，若这个operator放在最后，就有终结pipe的能力
    filter([expression | function]) -> 返回boolean，如果是true，留下符合条件的数据
)



